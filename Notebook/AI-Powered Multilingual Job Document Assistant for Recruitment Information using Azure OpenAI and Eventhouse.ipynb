{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cde35d",
   "metadata": {},
   "source": [
    "### Install required Python libraries for OpenAI, LangChain, Azure Kusto, and data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==1.12.0 azure-kusto-data langchain tenacity langchain-openai pypdf\n",
    "%pip install beautifulsoup4 langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7764d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import textwrap\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from notebookutils import mssparkutils\n",
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from azure.kusto.data.exceptions import KustoServiceError\n",
    "from azure.kusto.data.helpers import dataframe_from_result_table\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from bs4 import SoupStrainer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ba50a",
   "metadata": {},
   "source": [
    "### Initialize OpenAI client, generate text embeddings, and connect to Azure Data Explorer (Kusto) database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa34ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_GPT4_DEPLOYMENT_NAME=\"gpt-4o-shivam-project\"\n",
    "OPENAI_DEPLOYMENT_ENDPOINT=\"Replace with your OpenAI endpoint\" \n",
    "OPENAI_API_KEY=\"Replace with your OpenAI API key\" \n",
    "TEXTEMBEDDING_DEPLOYMENT_ENDPOINT=\"Replace with your OpenAI text embedding endpoint\" \n",
    "TEXTEMBEDDING_API_KEY=\"Replace with your OpenAI API key\" \n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada-002-shivam-project\"\n",
    "\n",
    "\n",
    "KUSTO_URL = 'Replace with your kusto URI' \n",
    "KUSTO_DATABASES = \"VectorDatabase\"\n",
    "KUSTO_TABLES = \"embeddingtables\"\n",
    "accessToken = mssparkutils.credentials.getToken(KUSTO_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646af25",
   "metadata": {},
   "source": [
    "### Initialize the Azure OpenAI client using endpoint, API key, and version details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openAI  = AzureOpenAI(\n",
    "        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        api_version=\"2025-01-01-preview\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35329712",
   "metadata": {},
   "source": [
    "### Create Azure OpenAI embedding client and define a retry-enabled function to generate text embeddings using Tenacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client= AzureOpenAI(\n",
    "        azure_endpoint=TEXTEMBEDDING_DEPLOYMENT_ENDPOINT,\n",
    "        api_key=TEXTEMBEDDING_API_KEY,\n",
    "        api_version=\"2023-05-15\"\n",
    "    )\n",
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text): \n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [txt], model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488380e",
   "metadata": {},
   "source": [
    "### Import necessary libraries and configure a text splitter to divide PDF content into manageable chunks for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, hashlib, json\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Splitter settings\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cfdf0",
   "metadata": {},
   "source": [
    "### Retrieve and list all PDF files from the specified directory for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PDF list\n",
    "import os\n",
    "\n",
    "# Folder where PDFs are uploaded\n",
    "base_path = \"/lakehouse/default/Files\"\n",
    "\n",
    "# List all PDF files dynamically\n",
    "pdf_files = [\n",
    "    {\n",
    "        \"name\": f,\n",
    "        \"path\": os.path.join(base_path, f),\n",
    "        \"source_url\": None\n",
    "    }\n",
    "    for f in os.listdir(base_path)\n",
    "    if f.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "print(\"Detected PDF files:\", [f[\"name\"] for f in pdf_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfebf04",
   "metadata": {},
   "source": [
    "### Extract tables from PDFs using pdfplumber and convert them into clean, compact Markdown format for embedding or text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de204984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- SAFE table -> markdown ---\n",
    "def df_to_markdown(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Turn a table dataframe into a compact markdown string for embeddings,\n",
    "    robust to None headers/cells.\"\"\"\n",
    "    # Make a copy so we don't mutate the caller's DF\n",
    "    df = df.copy()\n",
    "\n",
    "    # Clean headers: replace None with \"\" and cast to str\n",
    "    df.columns = [\"\" if c is None else str(c) for c in df.columns.tolist()]\n",
    "\n",
    "    # Clean cells: replace None/NaN with \"\", cast to str, collapse whitespace\n",
    "    df = df.fillna(\"\")\n",
    "    df = df.applymap(lambda x: re.sub(r\"\\s+\", \" \", \"\" if x is None else str(x)).strip())\n",
    "\n",
    "    header = \" | \".join(df.columns)\n",
    "    rows = [\" | \".join(row.tolist()) for _, row in df.iterrows()]\n",
    "    body = \"\\n\".join(rows)\n",
    "\n",
    "    return f\"TABLE\\n{header}\\n{body}\"\n",
    "\n",
    "\n",
    "def extract_tables_with_pdfplumber(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"Return a list of dicts: [{'page_no': int, 'markdown': str}, ...] for every table found.\"\"\"\n",
    "    out = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for p_idx, page in enumerate(pdf.pages, start=1):\n",
    "            # try both table finders\n",
    "            for table in page.extract_tables(table_settings={\"vertical_strategy\": \"lines\",\n",
    "                                                             \"horizontal_strategy\": \"lines\"}) or []:\n",
    "                if not table or len(table) < 2:\n",
    "                    continue\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                out.append({\"page_no\": p_idx, \"markdown\": df_to_markdown(df)})\n",
    "\n",
    "            # fallback stream mode\n",
    "            for table in page.extract_tables(table_settings={\"vertical_strategy\": \"text\",\n",
    "                                                             \"horizontal_strategy\": \"text\"}) or []:\n",
    "                if not table or len(table) < 2:\n",
    "                    continue\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                out.append({\"page_no\": p_idx, \"markdown\": df_to_markdown(df)})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3c5e9",
   "metadata": {},
   "source": [
    "### Define utility functions for generating the current UTC timestamp and detecting language (English or Hindi) based on character patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "def now_utc_iso() -> str:\n",
    "    # ISO8601; we’ll cast to timestamp before writing\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def detect_lang_simple(text: str) -> str:\n",
    "    \"\"\"Very light/fast detector: 'hi' if Devanagari chars appear, else 'en'.\"\"\"\n",
    "    if re.search(r\"[\\u0900-\\u097F]\", text or \"\"):\n",
    "        return \"hi\"\n",
    "    return \"en\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951bf40",
   "metadata": {},
   "source": [
    "### Load PDFs, split into text/table chunks, assign stable chunk_ids, and collect metadata (page, type, lang, timestamps) for indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, split, and index chunks  +  ADD chunk_id  ✱✱✱\n",
    "rows = []\n",
    "doc_ids_in_batch = set()   # ✱✱✱ keep track of doc_ids we touch in this run\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    try:\n",
    "        # -------- IDs / metadata --------\n",
    "        doc_id = hashlib.md5(pdf[\"name\"].encode(\"utf-8\")).hexdigest()\n",
    "        doc_ids_in_batch.add(doc_id)    # ✱✱✱\n",
    "        src_url = pdf.get(\"source_url\") or pdf[\"path\"]\n",
    "\n",
    "        # -------- text pages --------\n",
    "        loader = PyPDFLoader(pdf[\"path\"])\n",
    "        docs = loader.load()  # text only\n",
    "        chunk_idx = 0\n",
    "        text_chunks = 0\n",
    "        table_chunks = 0\n",
    "\n",
    "        for d in docs:\n",
    "            # PyPDFLoader pages are often 0-based; add 1 if you prefer 1-based page numbers.\n",
    "            page_no = int(d.metadata.get(\"page\", 0)) + 1\n",
    "            page_chunks = splitter.split_text(d.page_content)\n",
    "\n",
    "            for chunk_text in page_chunks:\n",
    "                # stable chunk_id: doc_id + page_no + chunk_idx + prefix of text\n",
    "                chunk_id = hashlib.md5(\n",
    "                    f\"{doc_id}|{page_no}|{chunk_idx}|{chunk_text[:128]}\".encode(\"utf-8\")\n",
    "                ).hexdigest()\n",
    "\n",
    "                rows.append({\n",
    "                    \"chunk_id\":      chunk_id,        # ✱✱✱ NEW\n",
    "                    \"doc_id\":        doc_id,\n",
    "                    \"document_name\": pdf[\"name\"],\n",
    "                    \"source_url\":    src_url,\n",
    "                    \"page_no\":       page_no,\n",
    "                    \"chunk_no\":      chunk_idx,\n",
    "                    \"content\":       chunk_text,\n",
    "                    \"content_type\":  \"text\",\n",
    "                    \"ingest_time\":   now_utc_iso(),\n",
    "                    \"lang\":          detect_lang_simple(chunk_text)\n",
    "                })\n",
    "                chunk_idx += 1\n",
    "                text_chunks += 1\n",
    "\n",
    "        # -------- tables (extra chunks) --------\n",
    "        table_items = extract_tables_with_pdfplumber(pdf[\"path\"])  # [{'page_no', 'markdown'}, ...]\n",
    "        for t in table_items:\n",
    "            page_no = t[\"page_no\"]  # already 1-based in our helper\n",
    "            # split table markdown too (tables can be large)\n",
    "            for chunk_text in splitter.split_text(t[\"markdown\"]):\n",
    "                chunk_id = hashlib.md5(\n",
    "                    f\"{doc_id}|{page_no}|{chunk_idx}|{chunk_text[:128]}\".encode(\"utf-8\")\n",
    "                ).hexdigest()\n",
    "\n",
    "                rows.append({\n",
    "                    \"chunk_id\":      chunk_id,\n",
    "                    \"doc_id\":        doc_id,\n",
    "                    \"document_name\": pdf[\"name\"],\n",
    "                    \"source_url\":    src_url,\n",
    "                    \"page_no\":       page_no,\n",
    "                    \"chunk_no\":      chunk_idx,\n",
    "                    \"content\":       chunk_text,\n",
    "                    \"content_type\":  \"table\",\n",
    "                    \"ingest_time\":   now_utc_iso(),\n",
    "                    \"lang\":          detect_lang_simple(chunk_text)\n",
    "                })\n",
    "                chunk_idx += 1\n",
    "                table_chunks += 1\n",
    "\n",
    "        print(f\"Loaded {text_chunks} text + {table_chunks} table chunks \"\n",
    "              f\"= {chunk_idx} total from {pdf['name']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {pdf['name']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f43d24",
   "metadata": {},
   "source": [
    "### Convert processed chunks into a DataFrame and verify data availability before proceeding to the next stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ✱✱✱ If nothing to process, bail out early\n",
    "if df.empty:\n",
    "    print(\"No chunks produced. Nothing to write.\")\n",
    "else:\n",
    "    print(f\"Prepared {len(df)} chunks in memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0a849",
   "metadata": {},
   "source": [
    "### De-duplicate chunks against Kusto by chunk_id, filter new rows, and generate embeddings only for the new chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────\n",
    "# DEDUP: remove rows that already exist in Eventhouse by chunk_id ✱✱✱\n",
    "#    (We only pull existing IDs for the doc_ids we have in this batch)\n",
    "# ─────────────────────────────────────────────────────────\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Build a small KQL to get existing chunk_ids for these doc_ids\n",
    "doc_ids_list = list(doc_ids_in_batch)\n",
    "# Guard: if huge list, you could write them to a temp table; usually it's small.\n",
    "doc_ids_literal = \",\".join([f\"'{d}'\" for d in doc_ids_list]) or \"''\"\n",
    "\n",
    "existing_ids_query = f\"\"\"\n",
    "{KUSTO_TABLES}\n",
    "| where doc_id in ({doc_ids_literal})\n",
    "| project chunk_id\n",
    "\"\"\"\n",
    "\n",
    "existing_ids_sdf = (\n",
    "    spark.read\n",
    "    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "    .option(\"kustoCluster\", KUSTO_URL)\n",
    "    .option(\"kustoDatabase\", KUSTO_DATABASES)\n",
    "    .option(\"accessToken\", accessToken)\n",
    "    .option(\"kustoQuery\", existing_ids_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Bring only the necessary column to driver as a set (usually small per doc)\n",
    "existing_ids = set()\n",
    "if existing_ids_sdf.columns:\n",
    "    existing_ids = set([r[\"chunk_id\"] for r in existing_ids_sdf.select(\"chunk_id\").distinct().collect()])\n",
    "\n",
    "print(f\"Found {len(existing_ids)} existing chunks in Eventhouse for these docs.\")\n",
    "\n",
    "# Filter out duplicates locally\n",
    "if existing_ids:\n",
    "    df = df[~df[\"chunk_id\"].isin(existing_ids)]\n",
    "\n",
    "print(f\"⚙️ New chunks to embed & write: {len(df)}\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"✅ Nothing new to insert. Skipping embeddings/write.\")\n",
    "else:\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 6) Embeddings (only for NEW rows)\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"Generating embeddings for new chunks...\")\n",
    "\n",
    "    def _embed_one(text):\n",
    "        try:\n",
    "            return generate_embeddings(text)\n",
    "        except Exception:\n",
    "            return [0.0]\n",
    "\n",
    "    df[\"embedding_list\"] = df[\"content\"].apply(_embed_one)\n",
    "    df[\"embedding\"] = df[\"embedding_list\"].apply(lambda v: json.dumps(v, ensure_ascii=False))\n",
    "    df.drop(columns=[\"embedding_list\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad28b60",
   "metadata": {},
   "source": [
    "### Cast fields, align schema, and append new embedded chunks to Azure Data Explorer (Kusto) via Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6613b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "if df.empty:\n",
    "    print(\"✅ Nothing new to insert. Skipping Spark write.\")\n",
    "else:\n",
    "    # build Spark DF\n",
    "    df_sp = spark.createDataFrame(df) \\\n",
    "        .withColumn(\"page_no\", col(\"page_no\").cast(\"int\")) \\\n",
    "        .withColumn(\"chunk_no\", col(\"chunk_no\").cast(\"int\")) \\\n",
    "        .withColumn(\"embedding\", col(\"embedding\").cast(\"string\"))\\\n",
    "        .withColumn(\"ingest_time\", to_timestamp(col(\"ingest_time\")))\n",
    "\n",
    "    # IMPORTANT: reorder columns to match Kusto table schema\n",
    "    ordered_cols = [\n",
    "        \"doc_id\",\n",
    "        \"document_name\",\n",
    "        \"source_url\",\n",
    "        \"page_no\",\n",
    "        \"chunk_no\",\n",
    "        \"content\",\n",
    "        \"embedding\",\n",
    "        \"chunk_id\",\n",
    "        \"content_type\",\n",
    "        \"ingest_time\",\n",
    "        \"lang\",\n",
    "    ]\n",
    "    df_sp = df_sp.select(*ordered_cols)\n",
    "\n",
    "    # write\n",
    "    (df_sp.write\n",
    "        .format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "        .option(\"kustoCluster\", KUSTO_URL)\n",
    "        .option(\"kustoDatabase\", KUSTO_DATABASES)\n",
    "        .option(\"kustoTable\", KUSTO_TABLES)\n",
    "        .option(\"accessToken\", accessToken)\n",
    "        .mode(\"Append\")\n",
    "        .save())\n",
    "    print(f\"✅ Wrote {df_sp.count()} new rows to Eventhouse.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab531576",
   "metadata": {},
   "source": [
    "### Retrieve relevant context from Kusto by vector similarity (cosine) and return surrounding chunks for a given question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_eventhousess(question, k=3, before=3, after=20):\n",
    "    emb = generate_embeddings(question)\n",
    "    emb_json = json.dumps(emb)\n",
    "\n",
    "    kql = f\"\"\"\n",
    "let q = dynamic({emb_json});\n",
    "let hits =\n",
    "{KUSTO_TABLES}\n",
    "| extend sim = series_cosine_similarity(q, todynamic(embedding))\n",
    "| top {k} by sim desc\n",
    "| project doc_id, hit_chunk = chunk_no;   // rename to avoid chunk_no1\n",
    "\n",
    "{KUSTO_TABLES}\n",
    "| join kind=inner (hits) on doc_id\n",
    "| where chunk_no between (hit_chunk - {before} .. hit_chunk + {after})\n",
    "| order by doc_id asc, chunk_no asc\n",
    "| project doc_id, document_name, source_url, page_no, chunk_no, content\n",
    "\"\"\"\n",
    "\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "        .option(\"kustoCluster\", KUSTO_URL)\n",
    "        .option(\"kustoDatabase\", KUSTO_DATABASES)\n",
    "        .option(\"accessToken\", accessToken)\n",
    "        .option(\"kustoQuery\", kql)\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379090",
   "metadata": {},
   "source": [
    "### Generate responses from Azure OpenAI GPT model using chat completions API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openAI(text):\n",
    "    response = client_openAI.chat.completions.create(\n",
    "        model=OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef702129",
   "metadata": {},
   "source": [
    "### Retrieve contextual chunks from Kusto, construct a prompt, and generate a final answer using Azure OpenAI GPT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "question = \" What is the On-line registration of Application by Candidates for IBPS Common Recruitment Process for Recruitment of Officers (Scale-I, II & III) and Office Assistants (Multipurpose) in Regional Rural Banks (RRBs) ?\"\n",
    "\n",
    "answers_df = get_answer_from_eventhousess(question, k)\n",
    "\n",
    "# Combine content\n",
    "answer = \" \".join([row['content'] for row in answers_df.rdd.toLocalIterator() if row['content']])\n",
    "\n",
    "# Create chat prompt\n",
    "prompt = f\"Question: {question}\\nInformation: {answer}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use only the provided information to answer. If the answer is not in the context, say 'I don't know'.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63106cae",
   "metadata": {},
   "source": [
    "### Detect language using `langdetect` and translate text to a target language with Azure OpenAI GPT for lightweight translation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langdetect (once, outside Fabric if needed)\n",
    "def detect_lang(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text)  # e.g. 'hi' for Hindi, 'en' for English\n",
    "    except:\n",
    "        return \"en\"\n",
    "\n",
    "def translate(text: str, target_lang: str) -> str:\n",
    "    \"\"\"Lightweight GPT translation. For heavy prod traffic, consider Azure Translator API.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    system = \"You are a precise translator. Translate the text to the target language only. No explanations.\"\n",
    "    user = f\"Target language: {target_lang}\\nText:\\n{text}\"\n",
    "    r = client_openAI.chat.completions.create(\n",
    "        model=OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b7734",
   "metadata": {},
   "source": [
    "### Retrieve multilingual answers from Kusto, translate if needed, and generate the final response in the user’s original language using Azure OpenAI GPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves 2 answers from Eventhouse\n",
    "nr_of_answers = 1\n",
    "question = \" Dy. Director (Architect) ka qualification kya hai ?\"\n",
    "src_lang = detect_lang(question)        # 'hi' / 'en' / etc.\n",
    "q_for_retrieval = question if src_lang == \"en\" else translate(user_q, \"English\")\n",
    "answers_df = get_answer_from_eventhousess(question, nr_of_answers)\n",
    "\n",
    "# Concatenates the answers\n",
    "answer = \"\"\n",
    "for row in answers_df.rdd.toLocalIterator():\n",
    "    answer = answer + \" \" + row['content']\n",
    "\n",
    "# Creates a prompt for GPT4 with the question and the 2 answers\n",
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "\n",
    "result = call_openAI(messages)\n",
    "final_answer = result if src_lang == \"en\" else translate(answer_en_or_hi, \"Hindi\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
